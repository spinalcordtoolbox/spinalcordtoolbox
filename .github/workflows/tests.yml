name: Tests

# EVENT TRIGGERS COPIED FROM CURRENTLY OPEN PR https://github.com/neuropoly/spinalcordtoolbox/pull/3125
on:
  push:
    branches:
      - master
      - release
  pull_request:
    branches:
      - '*'
  schedule:
    # https://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-syntax-for-github-actions#onschedule
    # > Scheduled workflows run on the latest commit on the default or base branch
    # i.e. this can only run on master
    - cron:  '0 11 * * *'

jobs:
  batch_processing:
    name: Test batch_processing.sh
    runs-on: ubuntu-18.04
    steps:
      - name: Checkout SCT
        uses: actions/checkout@v2

      # We use the REST API to fetch the most recently uploaded results from batch_processing.
      # jq and awk are used to filter the JSON response by artifact ID, which is then used to download the right .zip.
      # Relevant documentation: https://docs.github.com/en/rest/reference/actions#artifacts
      - name: Fetch past results
        run: |
          export NEWEST_ARTIFACT_ID=$(curl -H "Accept: application/vnd.github.v3+json" \
                                           https://api.github.com/repos/neuropoly/spinalcordtoolbox/actions/artifacts |
                                      jq '.artifacts[].id' |
                                      awk '{for(i=1;i<=NF;i++) if($i>maxval) maxval=$i;}; END { print maxval;}')
          curl -u admin:${{ secrets.GITHUB_TOKEN }} \
               -H "Accept: application/vnd.github.v3+json" \
               -L "https://api.github.com/repos/neuropoly/spinalcordtoolbox/actions/artifacts/$NEWEST_ARTIFACT_ID/zip" \
               --output batch_processing_results.zip

      # install_sct edits ~/.bashrc, but those environment changes don't get passed to subsequent steps in GH Actions.
      # So, we filter through the .bashrc and pass the values to $GITHUB_ENV and $GITHUB_PATH.
      # Relevant documentation: https://docs.github.com/en/actions/reference/workflow-commands-for-github-actions#environment-files
      - name: Install SCT
        run: |
          PIP_PROGRESS_BAR=off ./install_sct -y
          cat ~/.bashrc | grep "export SCT_DIR" | cut -d " " -f 2 >> $GITHUB_ENV
          cat ~/.bashrc | grep "export MPLBACKEND" | cut -d " " -f 2 >> $GITHUB_ENV
          cat ~/.bashrc | grep "export PATH" | grep -o "/.*" | cut -d ':' -f 1 >> $GITHUB_PATH

      - name: Run batch_processing.sh
        run: |
          ./batch_processing.sh

      - name: Validate results
        run: |
          source python/etc/profile.d/conda.sh
          conda activate venv_sct
          pytest unit_testing/test_batch_processing.py

      - name: Upload results
        uses: actions/upload-artifact@v2
        # Only upload new results on merged pull requests. This allows us to A) refresh the 90-day
        # retention period, and B) update the stored values if they've changed -- assuming that the
        # change was expected (e.g. if we've made a functionality change to one of our scripts).
        if: github.event.pull_request.merged == true  # TOOD: Does this work as expected? Double-check "on:" triggers
        with:
          name: batch-processing-results
          path: |
            sct_example_data/t2/csa_c2c3.csv
            sct_example_data/mt/mtr_in_wm.csv
            sct_example_data/t2s/csa_gm.csv
            sct_example_data/t2s/csa_wm.csv
            sct_example_data/dmri/fa_in_cst.csv